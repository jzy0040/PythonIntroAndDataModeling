{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528a8625-99a3-411a-a203-e75d0f6e197b",
   "metadata": {},
   "source": [
    "# Decision Making --- Reinforcement Learning\n",
    "Reinforcement Learning (RL) can be defined as the study of taking optimal decisions based on a Action-Reward mechanism. It is mainly intended to solve a specific kind of problem where the decision making is successive and the goal or objective is long-term, this includes robotics, game playing, or even logistics and resource management.\n",
    "\n",
    "In simple words, unlike the other machine learning algorithms, the ultimate goal of RL is not to be greedy all the time by looking for quick immediate rewards, rather optimize for maximum rewards over the complete training process. The following picture gives you an idea of the machine learning family. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"MachineLearning.png\" alt=\"Alt Text\" width=\"40%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea2a5b-4295-4f00-9e01-6357900135fc",
   "metadata": {},
   "source": [
    "### RL Terminologies\n",
    "\n",
    "Agent: The agent in RL can be defined as the entity which acts as a learner and decision-maker. It is empowered to interact continually, select its own actions, and respond to those actions.\n",
    "\n",
    "Environment: It is the abstract world through which the agent moves. The Environment takes the current state and action of the agent as input and returns its next state and appropriate reward as the output.\n",
    "\n",
    "States: The specific place at which an agent is present is called a state. This can either be the current situation of the agent in the environment or any of the future situations.\n",
    "\n",
    "Actions: This defines the set of all possible moves an agent can make within an environment.\n",
    "\n",
    "Reward or Penalty: This is nothing but the feedback by means of which the success or failure of an agent’s action within a given state can be measured. The rewards are used for the effective evaluation of an agent’s action.\n",
    "\n",
    "Policy or Strategy: It is mainly used to map the states along with their actions. The agent is said to use a strategy to determine the next best action based on its current state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43fddd-874e-40f8-afac-8288b43f086f",
   "metadata": {},
   "source": [
    "### What is Q-learning?\n",
    "\n",
    "Q-learning is a type of machine learning, other than Supervised and Unsupervised Learning, that helps a computer agent make decisions. It learns by trying different actions and determining which leads to the best results. Over time, the agent becomes smarter, making better choices based on gained experiences.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"ReinforcementLearning.jpg\" alt=\"Alt Text\" width=\"40%\">\n",
    "</div>\n",
    "\n",
    "### How does Q-learning work?\n",
    "Q-learning works by having an agent learn from its actions in an environment. The agent explores different actions, receives feedback on rewards or penalties, and adjusts its strategy to maximize cumulative rewards over time. This iterative process helps the agent make optimal decisions in various situations within the given environment.\n",
    "\n",
    "Q-learning is a model-free, value-based, off-policy learning algorithm.\n",
    "\n",
    "Model-free: The algorithm that estimates its optimal policy without the need for any transition or reward functions from the environment.\n",
    "Value-based: Q learning updates its value functions based on equations, (say Bellman equation) rather than estimating the value function with a greedy policy.\n",
    "\n",
    "Off-policy: The function learns from its own actions and doesn’t depend on the current policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d64b0-e0bf-478b-8384-b7c3f4b412fd",
   "metadata": {},
   "source": [
    "### Python Case Study --- Robots in a Warehouse\n",
    "\n",
    "The growing AI application in a new warehouse, for example, Sam's Club, would like all of the picking operations in the new warehouse to be performed by warehouse robots.\n",
    "\n",
    "After picking items from the shelves, the robots must bring the items to a specific location within the warehouse where the items can be packaged for shipping.\n",
    "\n",
    "In order to ensure maximum efficiency and productivity, the robots will need to learn the shortest path between the item packaging area and all other locations within the warehouse where the robots are allowed to travel.\r\n",
    "\r\n",
    "We will use Q-learning to accomplish this task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ec06206-bc3c-474c-8377-3a346c7b8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029cdaff-22ed-4b15-8d53-5a993e0d8e2f",
   "metadata": {},
   "source": [
    "### Define the RL Components\n",
    "1. State: The states in the environment are all of the possible locations within the warehouse. Some of these locations are for storing items (black squares), while other locations are aisles that the robot can use to travel throughout the warehouse (white squares). The green square indicates the item packaging and shipping area.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"WarehouseMap.png\" alt=\"Alt Text\" width=\"40%\">\n",
    "</div>\n",
    "\n",
    "The agent’s goal is to learn the shortest path between the item packaging area and all of the other locations in the warehouse where the robot is allowed to travel.\n",
    "\n",
    "2. Actions: The actions that are available to the agent are to move the robot in one of four directions: Up, Right, Down, Left. \n",
    "\n",
    "Obviously, the agent must learn to avoid driving into the item storage locations (e.g., shelves)!\n",
    "\n",
    "3. Rewards: The last component of the environment that we need to define is the rewards. To help the agent learn, each state (location) in the warehouse is assigned a reward value. The agent may begin at any white square, but its goal is always the same: to maximize its total rewards! Negative rewards, Aka, punishments, are used for all states except the goal.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"RewardsMap.png\" alt=\"Alt Text\" width=\"40%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d548ec30-2eb9-4810-b6c6-3a47d32e87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define actions\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a76913-d993-4e19-9764-222f920ec3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100. -100. -100. -100. -100.  100. -100. -100. -100. -100. -100.]\n",
      "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
      "[-100.   -1. -100. -100. -100. -100. -100.   -1. -100.   -1. -100.]\n",
      "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.   -1. -100.]\n",
      "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-100. -100. -100. -100. -100.   -1. -100. -100. -100. -100. -100.]\n",
      "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
      "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]\n"
     ]
    }
   ],
   "source": [
    "#Create a 2D numpy array to hold the rewards for each state. \n",
    "#The array contains 11 rows and 11 columns (to match the shape of the environment), and each value is initialized to -100.\n",
    "environment_rows = 11\n",
    "environment_columns = 11\n",
    "rewards = np.full((environment_rows, environment_columns), -100.)\n",
    "rewards[0, 5] = 100. #set the reward for the packaging area (i.e., the goal) to 100\n",
    "#define aisle locations (i.e., white squares) for rows 1 through 9\n",
    "aisles = {} #store locations in a dictionary\n",
    "aisles[1] = [i for i in range(1, 10)]\n",
    "aisles[2] = [1, 7, 9]\n",
    "aisles[3] = [i for i in range(1, 8)]\n",
    "aisles[3].append(9)\n",
    "aisles[4] = [3, 7]\n",
    "aisles[5] = [i for i in range(11)]\n",
    "aisles[6] = [5]\n",
    "aisles[7] = [i for i in range(1, 10)]\n",
    "aisles[8] = [3, 7]\n",
    "aisles[9] = [i for i in range(11)]\n",
    "#set the rewards for all aisle locations (i.e., white squares)\n",
    "for row_index in range(1, 10):\n",
    "  for column_index in aisles[row_index]:\n",
    "    rewards[row_index, column_index] = -1.\n",
    "#print rewards matrix\n",
    "for row in rewards:\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7761aa1-1323-4d76-a1e7-2714d701e071",
   "metadata": {},
   "source": [
    "### Train the RL model\n",
    "Our next task is for our agent to learn about its environment by implementing a Q-learning model. The learning process will follow these steps:\n",
    "\n",
    "1. Choose a random, non-terminal state (white square) for the agent to begin this new episode.\n",
    "   \n",
    "2. Choose an action (move up, right, down, or left) for the current state. Actions will be chosen using an epsilon greedy algorithm. This algorithm will usually choose the most promising action for the agent, but it will occasionally choose a less promising option in order to encourage the agent to explore the environment.\n",
    "   \n",
    "3. Perform the chosen action, and transition to the next state (i.e., move to the next location).\n",
    "   \n",
    "4. Receive the reward for moving to the new state, and calculate the temporal difference.\n",
    "   \n",
    "5. Update the Q-value for the previous state and action pair.\n",
    "    \n",
    "6. If the new (current) state is a terminal state, go to #1. Else, go to #2.\n",
    "   \n",
    "This entire process will be repeated across 1000 episodes. This will provide the agent sufficient opportunity to learn the shortest paths between the item packaging area and all other locations in the warehouse where the robot is allowed to travel, while simultaneously avoiding crashing into any of the item storage locations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6230c118-9f74-4ceb-944b-88cf95a9f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Helper Functions\n",
    "\n",
    "# define a function that determines if the specified location is a terminal state\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "  #if the reward for this location is -1, then it is not a terminal state (i.e., it is a 'white square')\n",
    "  if rewards[current_row_index, current_column_index] == -1.:\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "      \n",
    "# define a function that will choose a random, non-terminal starting location\n",
    "def get_starting_location():\n",
    "  #get a random row and column index\n",
    "  current_row_index = np.random.randint(environment_rows)\n",
    "  current_column_index = np.random.randint(environment_columns)\n",
    "  #continue choosing random row and column indexes until a non-terminal state is identified\n",
    "  #(i.e., until the chosen state is a 'white square').\n",
    "  while is_terminal_state(current_row_index, current_column_index):\n",
    "    current_row_index = np.random.randint(environment_rows)\n",
    "    current_column_index = np.random.randint(environment_columns)\n",
    "  return current_row_index, current_column_index\n",
    "    \n",
    "# define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "  #if a randomly chosen value between 0 and 1 is less than epsilon, \n",
    "  #then choose the most promising value from the Q-table for this state.\n",
    "  if np.random.random() < epsilon:\n",
    "    return np.argmax(q_values[current_row_index, current_column_index])\n",
    "  else: #choose a random action\n",
    "    return np.random.randint(4)\n",
    "# define a function that will get the next location based on the chosen action\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "  new_row_index = current_row_index\n",
    "  new_column_index = current_column_index\n",
    "  if actions[action_index] == 'up' and current_row_index > 0:\n",
    "    new_row_index -= 1\n",
    "  elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
    "    new_column_index += 1\n",
    "  elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
    "    new_row_index += 1\n",
    "  elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "    new_column_index -= 1\n",
    "  return new_row_index, new_column_index\n",
    "    \n",
    "# define a function that will get the shortest path between any location within the warehouse that \n",
    "# the robot is allowed to travel and the item packaging location.\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "  #return immediately if this is an invalid starting location\n",
    "  if is_terminal_state(start_row_index, start_column_index):\n",
    "    return []\n",
    "  else: #if this is a 'legal' starting location\n",
    "    current_row_index, current_column_index = start_row_index, start_column_index\n",
    "    shortest_path = []\n",
    "    shortest_path.append([current_row_index, current_column_index])\n",
    "    #continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
    "    while not is_terminal_state(current_row_index, current_column_index):\n",
    "      #get the best action to take\n",
    "      action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "      #move to the next location on the path, and add the new location to the list\n",
    "      current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "      shortest_path.append([current_row_index, current_column_index])\n",
    "    return shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69234ce5-cd87-4d8b-ae0a-887ef55a2ff5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#continue taking actions (i.e., moving) until we reach a terminal state\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#(i.e., until we reach the item packaging area or crash into an item storage location)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_terminal_state(row_index, column_index):\n\u001b[0;32m     13\u001b[0m   \u001b[38;5;66;03m#choose which action to take (i.e., where to move next)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m   action_index \u001b[38;5;241m=\u001b[39m get_next_action(row_index, column_index, epsilon)\n\u001b[0;32m     15\u001b[0m   \u001b[38;5;66;03m#perform the chosen action, and transition to the next state (i.e., move to the next location)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m   old_row_index, old_column_index \u001b[38;5;241m=\u001b[39m row_index, column_index \u001b[38;5;66;03m#store the old row and column indexes\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 28\u001b[0m, in \u001b[0;36mget_next_action\u001b[1;34m(current_row_index, current_column_index, epsilon)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_next_action\u001b[39m(current_row_index, current_column_index, epsilon):\n\u001b[0;32m     25\u001b[0m   \u001b[38;5;66;03m#if a randomly chosen value between 0 and 1 is less than epsilon, \u001b[39;00m\n\u001b[0;32m     26\u001b[0m   \u001b[38;5;66;03m#then choose the most promising value from the Q-table for this state.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m epsilon:\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(q_values[current_row_index, current_column_index])\n\u001b[0;32m     29\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#choose a random action\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'q_values' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the Agent using Q-Learning Algorithm\n",
    "#define training parameters\n",
    "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the agent should learn\n",
    "#run through 1000 training episodes\n",
    "for episode in range(1000):\n",
    "  #get the starting location for this episode\n",
    "  row_index, column_index = get_starting_location()\n",
    "  #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "  #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
    "  while not is_terminal_state(row_index, column_index):\n",
    "    #choose which action to take (i.e., where to move next)\n",
    "    action_index = get_next_action(row_index, column_index, epsilon)\n",
    "    #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "    old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "    row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "    reward = rewards[row_index, column_index]\n",
    "    old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "    temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "    #update the Q-value for the previous state and action pair\n",
    "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "    q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e270b-fba5-46e4-b3c6-33df36799ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Shortest Paths\n",
    "\n",
    "#display a few shortest paths\n",
    "print(get_shortest_path(3, 9)) #starting at row 3, column 9\n",
    "print(get_shortest_path(5, 0)) #starting at row 5, column 0\n",
    "print(get_shortest_path(9, 5)) #starting at row 9, column 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62fa64b8-29de-427f-96c6-4d0086310b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Shortest path from (9, 0) to goal:\n",
      "[[9, 0], [9, 1], [9, 2], [9, 3], [8, 3], [7, 3], [7, 4], [7, 5], [6, 5], [5, 5], [5, 6], [5, 7], [4, 7], [3, 7], [2, 7], [1, 7], [1, 6], [1, 5], [0, 5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define actions\n",
    "actions = ['up', 'right', 'down', 'left']\n",
    "\n",
    "# Environment setup\n",
    "environment_rows = 11\n",
    "environment_columns = 11\n",
    "rewards = np.full((environment_rows, environment_columns), -100.)\n",
    "rewards[0, 5] = 100.  # Packaging location\n",
    "\n",
    "# Aisle setup\n",
    "aisles = {\n",
    "    1: list(range(1, 10)),\n",
    "    2: [1, 7, 9],\n",
    "    3: list(range(1, 8)) + [9],\n",
    "    4: [3, 7],\n",
    "    5: list(range(11)),\n",
    "    6: [5],\n",
    "    7: list(range(1, 10)),\n",
    "    8: [3, 7],\n",
    "    9: list(range(11))\n",
    "}\n",
    "\n",
    "for row_index in range(1, 10):\n",
    "    for column_index in aisles[row_index]:\n",
    "        rewards[row_index, column_index] = -1.\n",
    "\n",
    "# Initialize Q-values (Q-table) with zeros: shape (rows, columns, actions)\n",
    "q_values = np.zeros((environment_rows, environment_columns, len(actions)))\n",
    "\n",
    "# Terminal state check\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "    return rewards[current_row_index, current_column_index] != -1.\n",
    "\n",
    "# Random valid starting position\n",
    "def get_starting_location():\n",
    "    while True:\n",
    "        row, col = np.random.randint(environment_rows), np.random.randint(environment_columns)\n",
    "        if not is_terminal_state(row, col):\n",
    "            return row, col\n",
    "\n",
    "# Epsilon-greedy action selection\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.argmax(q_values[current_row_index, current_column_index])\n",
    "    else:\n",
    "        return np.random.randint(len(actions))\n",
    "\n",
    "# Transition function\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "    new_row_index, new_column_index = current_row_index, current_column_index\n",
    "    if actions[action_index] == 'up' and current_row_index > 0:\n",
    "        new_row_index -= 1\n",
    "    elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
    "        new_column_index += 1\n",
    "    elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
    "        new_row_index += 1\n",
    "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "        new_column_index -= 1\n",
    "    return new_row_index, new_column_index\n",
    "\n",
    "# Trace shortest path\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "    if is_terminal_state(start_row_index, start_column_index):\n",
    "        return []\n",
    "    current_row_index, current_column_index = start_row_index, start_column_index\n",
    "    path = [[current_row_index, current_column_index]]\n",
    "    while not is_terminal_state(current_row_index, current_column_index):\n",
    "        action_index = get_next_action(current_row_index, current_column_index, 1.0)\n",
    "        current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "        path.append([current_row_index, current_column_index])\n",
    "    return path\n",
    "\n",
    "# Training\n",
    "epsilon = 0.9\n",
    "discount_factor = 0.9\n",
    "learning_rate = 0.9\n",
    "\n",
    "for episode in range(1000):\n",
    "    row_index, column_index = get_starting_location()\n",
    "    while not is_terminal_state(row_index, column_index):\n",
    "        action_index = get_next_action(row_index, column_index, epsilon)\n",
    "        old_row_index, old_column_index = row_index, column_index\n",
    "        row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "        reward = rewards[row_index, column_index]\n",
    "        old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "        temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "        new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "        q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "print('Training complete!')\n",
    "\n",
    "# Example: Show path from (9, 0)\n",
    "print(\"Shortest path from (9, 0) to goal:\")\n",
    "path = get_shortest_path(9, 0)\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02ac77-39aa-46dd-9ef2-3e89064950a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
